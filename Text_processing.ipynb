{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY8Iz5ALrnziZNXhSrGgpz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveenkarri383/NLP/blob/main/Text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CzDKC71Xqbmx"
      },
      "outputs": [],
      "source": [
        "# NLP --> Common Problem statements\n",
        "# Sentiment analysis, Language translation, Q and Answers , Text summerization, Named Entity Recognization, Sentence completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Importing required libraries\n",
        " import nltk\n",
        " import string\n",
        " import re  # regular expressions tokenizer"
      ],
      "metadata": {
        "id": "01iIjlw1vG9I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text lower case  # It is required for the text to reduce the size of the vocabulary of our text data\n",
        "def lowercase_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "input_str=\"Weather is too cloudy. So, there is a possibility of rain !!!\"\n",
        "lowercase_text(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TaY7JhYWyfb3",
        "outputId": "7c3288fa-c879-4d96-b8aa-cc8e2c276407"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'weather is too cloudy. so, there is a possibility of rain !!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removeing number # We should either remove or convert those into textual representations. use regular expressions to remove the number\n",
        "def remove_num(text):\n",
        "  result=re.sub(r'\\d+','',text)  # d+ is a pattern for any kind of number\n",
        "  return result\n",
        "\n",
        "input_str=\" I bough 6 apples and 1kg of oranges in the market!!!\"\n",
        "remove_num(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P83PHj28zGCy",
        "outputId": "4046d344-ad04-42ba-eb49-f7fb66205159"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I bough  apples and kg of oranges in the market!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numbers into words using ## Inflect library\n",
        "\n",
        "import inflect\n",
        "q=inflect.engine()\n",
        "\n",
        "# Converting number to text word\n",
        "def convert_num(text):\n",
        "    # split strings into list of texts\n",
        "    temp_str=text.split()\n",
        "    # Initialize empty list and add to that\n",
        "    new_str=[]\n",
        "\n",
        "    for word in temp_str:\n",
        "        if word.isdigit():   # if text is a digit then convert the digit into word and append into the new_str\n",
        "            temp=q.number_to_words(word)   # inflect library to convert the digit to text\n",
        "            new_str.append(temp)\n",
        "\n",
        "      # append the text as it is\n",
        "        else:\n",
        "           new_str.append(word)\n",
        "   # join the texts of new_str to form a string\n",
        "    temp_str=''.join(new_str)\n",
        "    return temp_str\n",
        "input_str=\" I bough 6 apples and 1kg of oranges in the market!!!\"\n",
        "convert_num(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mjt88Wji0SAP",
        "outputId": "6fbb8156-46a2-47a4-fcd8-e47b5c61c83e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Iboughsixapplesand1kgoforangesinthemarket!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuations\n",
        "def remove_punct(text):\n",
        "    translator=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "input_str=\"Hey there! I am using whatsapp!!!\"\n",
        "remove_punct(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TSK7TcLs2zRA",
        "outputId": "f21c3424-232c-4027-c8eb-0c7be2d6df69"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey there I am using whatsapp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removig Stopwords\n",
        "# Importing NLTK librarys\n",
        "from nltk.corpus import stopwords   # Corpous is a data set\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"stopwords\")  # These are very specific to type of language\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words=set(stopwords.words(\"english\"))  # stowords in english\n",
        "    word_tokens=word_tokenize(text)  # break sentence into words/tokens\n",
        "    filtered_text=[word for word in word_tokens if word not in stop_words]\n",
        "    return filtered_text\n",
        "\n",
        "input_text=\"AI can revolutionize the world. As AI develops the models becomes much faster\"\n",
        "remove_stopwords(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpdzFVQN4bKp",
        "outputId": "0e6a47b4-69df-42d9-dd05-c5d637a6145d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AI',\n",
              " 'revolutionize',\n",
              " 'world',\n",
              " '.',\n",
              " 'As',\n",
              " 'AI',\n",
              " 'develops',\n",
              " 'models',\n",
              " 'becomes',\n",
              " 'much',\n",
              " 'faster']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming --> Convert the words into base words  ex: Mango--> Mangoes , Going--> Go , Boys--> Boy\n",
        "#Stemming always converts the words into their root words\n",
        "from nltk.stem.porter import PorterStemmer   # other Stemming techniques are : Snowball , Lancaster\n",
        "from nltk.tokenize import word_tokenize\n",
        "stem1=PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    words_tokens=word_tokenize(text)\n",
        "    stems=[stem1.stem(word) for word in words_tokens]\n",
        "    return stems\n",
        "text=\"AI can revolutionize the world. As AI develops the models becomes much faster\"\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_OWG9mf6Dv8",
        "outputId": "8e05cbc9-3a5f-4628-d4f8-d2b4439d18c6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ai',\n",
              " 'can',\n",
              " 'revolution',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'as',\n",
              " 'ai',\n",
              " 'develop',\n",
              " 'the',\n",
              " 'model',\n",
              " 'becom',\n",
              " 'much',\n",
              " 'faster']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization -->  Convert word into nearest possible words which can have same meaning. It can also chop off the words\n",
        "from nltk.stem import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemma=wordnet.WordNetLemmatizer()\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Lemmatization of a string\n",
        "def lemmatize_word(text):\n",
        "    word_tokens=word_tokenize(text)\n",
        "\n",
        "    lemmas=[lemma.lemmatize(word,pos='v')for word in word_tokens]   # Parts of speech tagging to understand the text context\n",
        "    return lemmas\n",
        "\n",
        "text=\"AI can revolutionize the world. As AI develops the models becomes much faster\"\n",
        "lemmatize_word(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBjxexeh752j",
        "outputId": "23ea3761-8ba3-4206-dcb5-fd87e8dda09d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AI',\n",
              " 'can',\n",
              " 'revolutionize',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'As',\n",
              " 'AI',\n",
              " 'develop',\n",
              " 'the',\n",
              " 'model',\n",
              " 'become',\n",
              " 'much',\n",
              " 'faster']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts of speech tagging\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "# convert text into words\n",
        "def pos_tagger(text):\n",
        "    word_tokens=word_tokenize(text)\n",
        "    return pos_tag(word_tokens)\n",
        "\n",
        "text=\"AI can revolutionize the world. As AI develops the models becomes much faster\"\n",
        "pos_tagger(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L8WeIkC9niE",
        "outputId": "8dc23699-8aef-4eb8-88f8-bc5248033238"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AI', 'NNP'),\n",
              " ('can', 'MD'),\n",
              " ('revolutionize', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('world', 'NN'),\n",
              " ('.', '.'),\n",
              " ('As', 'IN'),\n",
              " ('AI', 'NNP'),\n",
              " ('develops', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('models', 'NNS'),\n",
              " ('becomes', 'RB'),\n",
              " ('much', 'JJ'),\n",
              " ('faster', 'RBR')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HgovaX9X-2FO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking --> Extracting phrases from the unstructured text and give them more structure to it.\n",
        "#We also call them shallow parsing. We can also do pos tagging on top of it.\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def chunking(text,grammer):\n",
        "    word_tokens=word_tokenize(text)\n",
        "\n",
        "    #label the word with pos taggging\n",
        "    word_pos=pos_tag(word_tokens)\n",
        "\n",
        "    # create a chunk parser using grammer\n",
        "    chunkParser= nltk.RegexpParser(grammer)\n",
        "\n",
        "    # test it on the list of word tokens with tagged pos\n",
        "    tree=chunkParser.parse(word_pos)\n",
        "\n",
        "    for subtree in tree.subtrees():\n",
        "        print(subtree)\n",
        "\n",
        "    # tree draw()\n",
        "\n",
        "sentence=\"The little red parrot is flying in the sky\"\n",
        "grammer=  \"NP: {<DT>?<JJ>*<NN>}\"     # regular expression\n",
        "chunking(sentence,grammer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfM1t54CAKjY",
        "outputId": "70802e61-d63a-4666-8059-28253a0a6b63"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP The/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognization--> It is used to extract information from unstructured text. It is used to classify the entities which is present in the text into\n",
        "# categories like persons, events, organizations etc..\n",
        "# It also gives the idea about the relationship between the entities\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag,ne_chunk\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "\n",
        "def ner(text):\n",
        "    word_tokens=word_tokenize(text)\n",
        "\n",
        "    word_pos=pos_tag(word_tokens)\n",
        "\n",
        "    print(ne_chunk(word_pos))\n",
        "\n",
        "text=  \"Porter stemmer. The Porter stemming algorithm is a process for removing suffixes from words in English. Removing suffixes. automatically is an operation which is especially useful in the field of information retrieval.\"\n",
        "ner(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtJGnGeVBmI6",
        "outputId": "14b71380-bef8-4957-a6a9-1c6412f23ce3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Porter/NNP)\n",
            "  stemmer/NN\n",
            "  ./.\n",
            "  The/DT\n",
            "  Porter/NNP\n",
            "  stemming/VBG\n",
            "  algorithm/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  process/NN\n",
            "  for/IN\n",
            "  removing/VBG\n",
            "  suffixes/NNS\n",
            "  from/IN\n",
            "  words/NNS\n",
            "  in/IN\n",
            "  (GPE English/NNP)\n",
            "  ./.\n",
            "  Removing/VBG\n",
            "  suffixes/NNS\n",
            "  ./.\n",
            "  automatically/RB\n",
            "  is/VBZ\n",
            "  an/DT\n",
            "  operation/NN\n",
            "  which/WDT\n",
            "  is/VBZ\n",
            "  especially/RB\n",
            "  useful/JJ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  information/NN\n",
            "  retrieval/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja37zE24EEsY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}